# -*- coding: utf-8 -*-
"""Moringa_Data_Science _Core_W1_Independent_Project_2021_03_Victor_Bundi_Financial_Inclusion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rqli1ChCwn77KLwmrScCYDVX0GpRk1Qr

#FINANCIAL INCLUSION DETERMINATION

**a) Data Analytic Questions**

  *Assess the financial inclusion status in select East African countries based on Finscope financial surveys data ranging from 2016 to 2018*
  *Identifying factors that influence whether an individual has a bank account or not.*


```
# This is formatted as code
```


**b) Success Metrics**
*Sucessful prediction of individuals most likely to have or use a bank account.**


```
# This is formatted as code
```


**c) context**
*Financial Inclusion remains one of the main obstacles to economic and human development in Africa.In the East African Region only 9.1 million adults (or 13.9% of the adult population) have access to or use a commercial bank account.Generally,Traditionally, access to bank accounts has been regarded as an indicator of financial inclusion.*


```
# This is formatted as code
```


**d) Experimental Design**

```
# This is formatted as code
```


*Formulation of the research question*

*Loading the data*

*Check the data*

*Validating the data*

*Formulate the solution*

*Challenge the solution*

*Follow up*

#DATA ANALYSIS AND MODELING

#Loading required libraries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib
from matplotlib import pyplot as plt

"""#Loading the Financial_Dataset"""

#Reading in the Data
Financial_Dataset = pd.read_csv('Financial_Dataset.csv')
Financial_Dataset.head()

"""#Check out the Data

*Check out variables types and descriptive statistics*
"""

#Determining the no. of records in our dataset.
Financial_Dataset.shape

#Check whether each column has an appropriate datatype.
Financial_Dataset.dtypes

#Previewing the top of our dataset
Financial_Dataset.head(3)

#Previewing the bottom of our dataset
Financial_Dataset.tail(3)

"""*Measures of central tendency*"""

#Preview each column and check the measures of central tendency
Financial_Dataset.describe(include='all')

"""#Tidying the Dataset

*Standardize column names*
"""

# Standardize column names

Financial_Dataset.columns = ['country','year','uniqueid','Has_a_Bank_account' ,'Type_of_Location',
'Cell_Phone_Access','household_size','Respondent_Age','gender_of_respondent',
'The_relathip_with_head','marital_status','Level_of_Educuation','Type_of_Job' ]
Financial_Dataset.head(3)

"""*Missing value analysis*"""

#Check if there are any null values in the dataset.

Financial_Dataset.isnull().values.any()

#Count the total number of missing values across the columns

Financial_Dataset.isnull().sum()

"""*Dealing with the Missing Data*"""

#Drop Rows with NaN Values  and check shape
Financial_Dataset = Financial_Dataset.dropna()
Financial_Dataset.shape

"""*Outlier detection analysis*"""

col_names= ['year','household_size','Respondent_Age']

fig, ax = plt.subplots(len(col_names), figsize=(8,20))

for i, col_val in enumerate(col_names):

    sns.boxplot(y=Financial_Dataset[col_val], ax=ax[i])
    ax[i].set_title('Box plot - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()

Financial_Dataset.shape

"""*Percentile based outlier removal*

"""

#Values marked with a red dot below in the x-axis of the graph are the ones that are removed
#from the column based on the set threshold percentile (95 in our case), and
#is also the default value when it comes to percentile-based outlier removal.

def percentile_based_outlier(data, threshold=99):
    diff = (100 - threshold) / 2
    minval, maxval = np.percentile(data, [diff, 100 - diff])
    return (data < minval) | (data > maxval)

col_names = ['year','household_size','Respondent_Age']

fig, ax = plt.subplots(len(col_names), figsize=(8,20))

for i, col_val in enumerate(col_names):
    x = Financial_Dataset[col_val][:1000]
    sns.distplot(x, ax=ax[i], rug=True, hist=False)
    outliers = x[percentile_based_outlier(x)]
    ax[i].plot(outliers, np.zeros_like(outliers), 'ro', clip_on=False)

    ax[i].set_title('Outlier detection - {}'.format(col_val), fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)

plt.show()

"""#UNIVARIATE AND BIVARIATE ANALYSIS

*Intercountry Comparison country*
"""

Financial_Dataset['country'].value_counts().plot.bar(title='Freq dist by country')

"""*year*"""

Financial_Dataset['year'].value_counts().plot.bar(title='Freq dist by year')

"""*Has a Bank account*"""

Financial_Dataset['Has_a_Bank_account'].value_counts().plot.bar(title='Freq dist by Bank Acoount Ownership')

"""*Type of Location*"""

Financial_Dataset['Type_of_Location'].value_counts().plot.bar(title='Freq dist by Location')

"""*Cell Phone Access*"""

Financial_Dataset['Cell_Phone_Access'].value_counts().plot.bar(title='Freq dist by Cell Phone Access')

"""*household_size*"""

Financial_Dataset['household_size'].value_counts().plot.bar(title='Freq dist by household_size')

"""*gender_of_respondent*"""

Financial_Dataset['gender_of_respondent'].value_counts().plot.bar(title='Freq dist by gender')

"""*marital_status*"""

Financial_Dataset['marital_status'].value_counts().plot.bar(title='Freq dist by marital_status')

"""*Level of Educuation*"""

Financial_Dataset['Level_of_Educuation'].value_counts().plot.bar(title='Freq dist by Level of Educuation')

"""*Type of Job*"""

Financial_Dataset['Type_of_Job'].value_counts().plot.bar(title='Freq dist by Type of Job')

"""#Crosstab Visualizations

*Account holders by Country*
"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.country).plot(kind='bar')
plt.title('Account holders by Country')
plt.xlabel('country')
plt.ylabel('Frequency')
plt.show()

"""*Account holders by Gender*"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.gender_of_respondent).plot(kind='bar')
plt.title('Account holders by Gender')
plt.xlabel('Gender')
plt.ylabel('Frequency')
plt.show()

"""*Account holders by marital status*"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.marital_status).plot(kind='bar')
plt.title('Account holders by marital status')
plt.xlabel('marital_status')
plt.ylabel('Frequency')
plt.show()

"""*Account holder by Level of Educuation*"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.Level_of_Educuation).plot(kind='bar')
plt.title('Account holders by Education Level')
plt.xlabel('Education Level')
plt.ylabel('Frequency')
plt.show()

"""Account holder by Cell Phone Access"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.Cell_Phone_Access).plot(kind='bar')
plt.title('Account holders by Cell Phone Access')
plt.xlabel('Cell Phone Access')
plt.ylabel('Frequency')
plt.show()

"""Account holder by Location"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.Type_of_Location).plot(kind='bar')
plt.title('Account holders by Location')
plt.xlabel('Location')
plt.ylabel('Frequency')
plt.show()

"""Acoount holder by Type of Job'"""

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.Type_of_Job).plot(kind='bar')
plt.title('Account holders by Type of Job')
plt.xlabel('Type of Job')
plt.ylabel('Frequency')
plt.show()

""" Account holder by household_size """

pd.crosstab(Financial_Dataset.Has_a_Bank_account,Financial_Dataset.household_size).plot(kind='bar')
plt.title('Account holders by household_size ')
plt.xlabel('household_size ')
plt.ylabel('Frequency')
plt.show()

"""#*univariate distribution of the numerical columns*"""

#We use displot of the seaborn library to plot this graph
col_names = ['year','household_size','Respondent_Age']

fig, ax = plt.subplots(len(col_names), figsize=(15,20))

for i, col_val in enumerate(col_names):

    sns.distplot(Financial_Dataset[col_val], hist=True, ax=ax[i])
    ax[i].set_title('Freq dist '+col_val, fontsize=10)
    ax[i].set_xlabel(col_val, fontsize=8)
    ax[i].set_ylabel('Count', fontsize=8)

plt.show()

"""#**Label Encoding**

**Convert categorical variables into numerical values for further processing.**
"""

#We start by checking variable types
Financial_Dataset.dtypes

# next we change variable types.
Financial_Dataset["Has_a_Bank_account"] = Financial_Dataset["Has_a_Bank_account"].astype('category')
Financial_Dataset["Type_of_Location"] = Financial_Dataset["Type_of_Location"].astype('category')
Financial_Dataset["Cell_Phone_Access"] = Financial_Dataset["Cell_Phone_Access"].astype('category')
Financial_Dataset["household_size"] = Financial_Dataset["household_size"].astype('int64')
Financial_Dataset["Respondent_Age"] = Financial_Dataset["Respondent_Age"].astype('int64')
Financial_Dataset["The_relathip_with_head"] = Financial_Dataset["The_relathip_with_head"].astype('category')
Financial_Dataset["gender_of_respondent"] = Financial_Dataset["gender_of_respondent"].astype('category')
Financial_Dataset["marital_status"] = Financial_Dataset["marital_status"].astype('category')
Financial_Dataset["Level_of_Educuation"] = Financial_Dataset["Level_of_Educuation"].astype('category')
Financial_Dataset["Type_of_Job"] = Financial_Dataset["Type_of_Job"].astype('category')

#Drop unneccessary columns
Financial_Dataset= Financial_Dataset.drop(['uniqueid','country','year'],axis=1)

#Next we encoded variable via the cat.codes accessor
Financial_Dataset["Has_a_Bank_account"] = Financial_Dataset["Has_a_Bank_account"].cat.codes
Financial_Dataset["Type_of_Location"] = Financial_Dataset["Type_of_Location"].cat.codes
Financial_Dataset["Cell_Phone_Access"] = Financial_Dataset["Cell_Phone_Access"].cat.codes
Financial_Dataset["The_relathip_with_head"] = Financial_Dataset["The_relathip_with_head"].cat.codes
Financial_Dataset["gender_of_respondent"] = Financial_Dataset["gender_of_respondent"].cat.codes
Financial_Dataset["marital_status"] = Financial_Dataset["marital_status"].cat.codes
Financial_Dataset["Level_of_Educuation"] = Financial_Dataset["Level_of_Educuation"].cat.codes
Financial_Dataset["Type_of_Job"] = Financial_Dataset["Type_of_Job"].cat.codes

Financial_Dataset.head()

"""#**Plots for the bivariate distributions**"""

#The bivariate distribution plots help us to study the relationship between two variables by analyzing the scatter plot, 
#and we use the pairplot() function of the seaborn package to plot the bivariate distributions.
sns.pairplot(Financial_Dataset)

"""**The correlation matrix**"""

#This analysis allows you to see which pairs have the highest correlation, the pairs which are highly correlated
#represent the same variance of the dataset thus we can further analyze them to understand which attribute among 
#the pairs are most  significant for building the model.

f ,ax = plt.subplots(figsize=(10, 8))
corr = Financial_Dataset.corr()
sns.heatmap(corr,
            xticklabels=corr.columns.values,
            yticklabels=corr.columns.values)

"""#Principal Component Analysis"""

# Step1:The first preprocessing step is to divide the dataset  into dependent and predictor variables
X = Financial_Dataset.drop('Has_a_Bank_account', 1)
y = Financial_Dataset['Has_a_Bank_account']

#Check x data
X.shape

#Check Y data
y.shape

# Step 2: Splitting the dataset into the Training set and Test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Step 3: Normalization
# We will perform standard scalar normalization to normalize our feature set. 

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# Step 4: Applying PCA
# Performing PCA using Scikit-Learn is a two-step process:
# Initialize the PCA class by passing the number of components to the constructor.
# Call the fit and then transform methods by passing the feature set to these methods. 
# The transform method returns the specified number of principal components.
# Let's take a look at the following code. In the code above, we create a PCA object named pca. 
# We did not specify the number of components in the constructor. 
# Hence, all four of the features in the feature set will be returned for both the training and test sets.

from sklearn.decomposition import PCA

pca = PCA()
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

# Step 5: Explained Variance Ratio
# The PCA class contains explained_variance_ratio_ which returns the variance caused by each of the principal components. 
explained_variance = pca.explained_variance_ratio_

# The explained_variance variable is now a float type array which contains variance ratios for each principal component. 
# The values for the explained_variance variable looks like what is shown in the output. 
# It can be seen that first principal component is responsible for 18.9% variance. 
# Similarly, the least principal component causes 5.05% variance in the dataset. 
explained_variance

# Step 6: Using 1 Principal Component
 
from sklearn.decomposition import PCA

pca = PCA(n_components=1)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)

# Step 7: Training and Making Predictions
# Via random forest classification for making the predictions.
from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)

# Predicting the Test set results
y_pred = classifier.predict(X_test)

# Step 8: Performance Evaluation
# 
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_test, y_pred)
print(cm)
print('Accuracy' , accuracy_score(y_test, y_pred))

# the random forest algorithm with only one feature, is able to correctly predict out put at 85.84% accuracy.

"""#**Factor Analysis**"""

STEP1
# Installing factor analyzer 
!pip install factor_analyzer==0.2.3

from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity

chi_square_value,p_value=calculate_bartlett_sphericity(Financial_Dataset)
chi_square_value, p_value

# In Bartlett â€™s test, the p-value is 0. The test was statistically significant, 
# indicating that the observed correlation matrix is not an identity matrix.

# Step 2:
# Kaiser-Meyer-Olkin (KMO) Test measures the suitability of data for factor analysis. 
# It determines the adequacy for each observed variable and for the complete model. 
# KMO estimates the proportion of variance among all the observed variable. 
# Lower proportion id more suitable for factor analysis. KMO values range between 0 and 1. 
# Value of KMO less than 0.6 is considered inadequate.
# 
from factor_analyzer.factor_analyzer import calculate_kmo

kmo_all,kmo_model=calculate_kmo(Financial_Dataset)

# The overall KMO for our data is 0.84, which is excellent. 
kmo_model

kmo_all

"""**Choosing the Number of Factors**"""

from factor_analyzer import FactorAnalyzer
# Creating factor analysis object and perform factor analysis
fa = FactorAnalyzer()
fa.analyze(Financial_Dataset, 10, rotation=None)

# Checking the Eigenvalues
ev, v = fa.get_eigenvalues()
ev

# Creating a scree plot using matplotlib
#
plt.scatter(range(1,Financial_Dataset.shape[1]+1),ev)
plt.plot(range(1,Financial_Dataset.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigenvalue')
plt.grid()
plt.show()

"""**Performing Factor Analysis**"""

# Creating factor analysis object and perform factor analysis
#
fa = FactorAnalyzer()
fa.analyze(Financial_Dataset, 9, rotation="varimax")
fa.loadings

"""**Getting variance of each factors**"""

fa.get_factor_variance()

"""# **Discriminant Analysis**"""

#Creating data frame
X = Financial_Dataset.iloc[:, 0:9].values
y = Financial_Dataset.iloc[:, 0].values

"""*Next divide the data into training and test sets*"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

"""*Feature scaling*"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

"""*Peforming LDA*"""

# The LinearDiscriminantAnalysis class of the sklearn.discriminant_analysis 

from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA
lda = LDA(n_components=1)
X_train = lda.fit_transform(X_train, y_train)
X_test = lda.transform(X_test)

"""*Training and Making Predictions*"""

from sklearn.ensemble import RandomForestClassifier

classifier = RandomForestClassifier(max_depth=2, random_state=0)
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)

y_pred

"""*Evaluating the Performance*"""

# with the help of a confusion matrix and we find the accuracy of the prediction.
# 

from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

cm = confusion_matrix(y_test, y_pred)
print(cm)
print('Accuracy' + str(accuracy_score(y_test, y_pred)))

"""*We can see that with one linear discriminant, the algorithm achieved an accuracy of 100%*

#Analysis Finings

* Residenscial location doest influece account ownership.
* Mobile phone access has great influence on bank account ownership.
* Level of education and type of job has influence on account ownership.

#*Challenging the solution*

*Further analysis could be carried out via (probit and Logistic) regression*

#*Follow up questions*
*At this point, we can refine our question or collect new data, all in an iterative process to get at the truth.*

* Did we have the right data? *yes*
* Do we need other data to answer our question? *yes*
"""